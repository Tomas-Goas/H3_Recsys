{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9RQZRGYjoKS"
      },
      "source": [
        "# Notebook GRU4Rec sin Dwell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AC0H8q0v5OAP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "nov = pd.read_csv(\"nov_reduced.csv\")\n",
        "\n",
        "# evento_time como datetime\n",
        "nov[\"event_time\"] = pd.to_datetime(nov[\"event_time\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hhui51GS3lgF"
      },
      "outputs": [],
      "source": [
        "def split_train_test(df):\n",
        "\n",
        "    df = df.sort_values(\"event_time\")\n",
        "    cutoff = df[\"event_time\"].quantile(0.9)\n",
        "\n",
        "    train = df[df[\"event_time\"] < cutoff]\n",
        "    test  = df[df[\"event_time\"] >= cutoff]\n",
        "\n",
        "    return train, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PeqdOELNHn-",
        "outputId": "2354934f-4e49-43a5-f0a1-533a549af39b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60751765 6750214\n"
          ]
        }
      ],
      "source": [
        "nov_train, nov_test = split_train_test(nov)\n",
        "print(len(nov_train), len(nov_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w_NkEURzOtJ4"
      },
      "outputs": [],
      "source": [
        "def build_sessions(df):\n",
        "    df = df.sort_values([\"user_session\", \"event_time\"])\n",
        "    sessions = df.groupby(\"user_session\")[\"product_id\"].apply(list)\n",
        "    return sessions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BtvurIj2OuTU"
      },
      "outputs": [],
      "source": [
        "nov_train_sessions = build_sessions(nov_train)\n",
        "nov_test_sessions  = build_sessions(nov_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7JKYO96XTRZ3"
      },
      "outputs": [],
      "source": [
        "def filter_sessions_min_length(sessions, min_len=5):\n",
        "    return [seq for seq in sessions if len(seq) >= min_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TyEWYkdLTSqT"
      },
      "outputs": [],
      "source": [
        "nov_train_sessions = filter_sessions_min_length(nov_train_sessions, min_len=5)\n",
        "nov_test_sessions  = filter_sessions_min_length(nov_test_sessions,  min_len=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7J-ErRXaO1Th"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_id_maps(sessions):\n",
        "    items = set()\n",
        "    for seq in sessions:\n",
        "        for item in seq:\n",
        "            items.add(item)\n",
        "\n",
        "    item2idx = {item: i+1 for i, item in enumerate(sorted(items))}\n",
        "    idx2item = {v: k for k, v in item2idx.items()}\n",
        "    return item2idx, idx2item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "i1Zw2zxjPaKT"
      },
      "outputs": [],
      "source": [
        "item2idx, idx2item = build_id_maps(nov_train_sessions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def filter_top_items(train_sessions, max_items=15000):\n",
        "    item_counts = Counter()\n",
        "    for session in train_sessions:\n",
        "        for item in session:\n",
        "            item_counts[item] += 1\n",
        "\n",
        "    top_items = set([item for item, _ in item_counts.most_common(max_items)])\n",
        "\n",
        "    filtered_sessions = []\n",
        "    for session in train_sessions:\n",
        "        filtered = [item for item in session if item in top_items]\n",
        "        if len(filtered) >= 5:\n",
        "            filtered_sessions.append(filtered)\n",
        "\n",
        "    return filtered_sessions, top_items\n",
        "\n",
        "nov_train_sessions, top_items = filter_top_items(nov_train_sessions, max_items=15000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "nov_test_sessions_filtered = []\n",
        "for session in nov_test_sessions:\n",
        "    filtered = [item for item in session if item in top_items]\n",
        "    if len(filtered) >= 5:\n",
        "        nov_test_sessions_filtered.append(filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "items_present = set(item for session in nov_train_sessions\n",
        "                    for item in session)\n",
        "\n",
        "item2idx_filtered = {item: i+1 for i, item in enumerate(sorted(items_present))}\n",
        "idx2item_filtered = {i: item for item, i in item2idx_filtered.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "A_ZMi88wPdK5"
      },
      "outputs": [],
      "source": [
        "def encode_sessions(sessions, item2idx, min_len=5):\n",
        "    encoded = []\n",
        "    for seq in sessions:\n",
        "        seq = [item2idx[x] for x in seq if x in item2idx]\n",
        "        if len(seq) >= min_len:\n",
        "            encoded.append(seq)\n",
        "    return encoded\n",
        "\n",
        "train_encoded = encode_sessions(nov_train_sessions, item2idx_filtered)\n",
        "test_encoded  = encode_sessions(nov_test_sessions, item2idx_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAHDk6UMc09R"
      },
      "source": [
        "### Modelo GRU sin Dwell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "htGmAq8JPeoM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "MAX_LEN = 15\n",
        "\n",
        "class GRUDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        if len(seq) > MAX_LEN:\n",
        "            seq = seq[-MAX_LEN:]   # conservar las últimas interacciones\n",
        "        return torch.tensor(seq[:-1]), torch.tensor(seq[1:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MgZn497-ST3O"
      },
      "outputs": [],
      "source": [
        "PAD_IDX = 0\n",
        "\n",
        "def collate_fn(batch):\n",
        "    X_batch, y_batch = zip(*batch)\n",
        "\n",
        "    # Longitudes individuales\n",
        "    lengths = [len(x) for x in X_batch]\n",
        "    max_len = min(MAX_LEN, max(lengths))\n",
        "\n",
        "    # Pading\n",
        "    X_padded = [torch.cat([x, torch.full((max_len - len(x),), PAD_IDX)]) for x in X_batch]\n",
        "    y_padded = [torch.cat([y, torch.full((max_len - len(y),), PAD_IDX)]) for y in y_batch]\n",
        "\n",
        "    return torch.stack(X_padded), torch.stack(y_padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de batches en train_loader: 96789\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    GRUDataset(train_encoded),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    GRUDataset(test_encoded),\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(\"Número de batches en train_loader:\", len(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "B7-1bpX8Phu6"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class GRU4Rec(nn.Module):\n",
        "    def __init__(self, n_items, emb_size=128, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(n_items+1, emb_size)\n",
        "        self.gru = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, n_items+1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        out, _ = self.gru(x)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9DVEc4HnPjGo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "n_items = len(item2idx)\n",
        "model = GRU4Rec(n_items).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UEy3rSekPkc8"
      },
      "outputs": [],
      "source": [
        "def train_gru(model, loader, optimizer, criterion, epochs=10):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        total_loss = 0\n",
        "        n_batch = 0\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(X)\n",
        "\n",
        "            logits = logits.reshape(-1, logits.size(-1))\n",
        "            y = y.reshape(-1)\n",
        "\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            n_batch += 1\n",
        "\n",
        "            print(f\"\\rEpoch {epoch} - Batch Loss: {loss.item():.4f} - N Batch: {n_batch}\", end=\"\")\n",
        "\n",
        "        print(f\"[Epoch {epoch}] Loss = {total_loss/len(loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Batch Loss: 3.8820 - N Batch: 96789[Epoch 1] Loss = 3.8155\n",
            "Epoch 2 - Batch Loss: 3.3484 - N Batch: 96789[Epoch 2] Loss = 3.5327\n",
            "Epoch 3 - Batch Loss: 3.0185 - N Batch: 96789[Epoch 3] Loss = 3.4961\n",
            "Epoch 4 - Batch Loss: 3.7615 - N Batch: 96789[Epoch 4] Loss = 3.4752\n",
            "Epoch 5 - Batch Loss: 3.0139 - N Batch: 96789[Epoch 5] Loss = 3.4605\n",
            "Epoch 6 - Batch Loss: 3.3034 - N Batch: 96789[Epoch 6] Loss = 3.4495\n",
            "Epoch 7 - Batch Loss: 4.0285 - N Batch: 96789[Epoch 7] Loss = 3.4416\n",
            "Epoch 8 - Batch Loss: 3.7380 - N Batch: 96789[Epoch 8] Loss = 3.4352\n",
            "Epoch 9 - Batch Loss: 3.3445 - N Batch: 96789[Epoch 9] Loss = 3.4301\n",
            "Epoch 10 - Batch Loss: 3.2809 - N Batch: 96789[Epoch 10] Loss = 3.4260\n"
          ]
        }
      ],
      "source": [
        "train_gru(model, train_loader, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, loader, k=10):\n",
        "    model.eval()\n",
        "    \n",
        "    recall_list = []\n",
        "    mrr_list = []\n",
        "    ndcg_list = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            \n",
        "            # Predecir\n",
        "            logits = model(X)  # (batch, seq_len, n_items)\n",
        "            \n",
        "            # Evaluar cada secuencia\n",
        "            for i in range(X.size(0)):\n",
        "                for j in range(X.size(1)):\n",
        "                    target = y[i, j].item()\n",
        "                    \n",
        "                    # Ignorar padding\n",
        "                    if target == PAD_IDX:\n",
        "                        continue\n",
        "                    \n",
        "                    # Obtener scores y top-k\n",
        "                    scores = logits[i, j]\n",
        "                    _, top_k_indices = torch.topk(scores, k)\n",
        "                    top_k_indices = top_k_indices.cpu().numpy()\n",
        "                    \n",
        "                    # Recall@k\n",
        "                    recall = 1.0 if target in top_k_indices else 0.0\n",
        "                    recall_list.append(recall)\n",
        "                    \n",
        "                    # MRR@k\n",
        "                    if target in top_k_indices:\n",
        "                        rank = np.where(top_k_indices == target)[0][0] + 1\n",
        "                        mrr_list.append(1.0 / rank)\n",
        "                    else:\n",
        "                        mrr_list.append(0.0)\n",
        "                    \n",
        "                    # NDCG@k\n",
        "                    if target in top_k_indices:\n",
        "                        rank = np.where(top_k_indices == target)[0][0] + 1\n",
        "                        ndcg_list.append(1.0 / np.log2(rank + 1))\n",
        "                    else:\n",
        "                        ndcg_list.append(0.0)\n",
        "    \n",
        "    recall_at_k = np.mean(recall_list)\n",
        "    mrr_at_k = np.mean(mrr_list)\n",
        "    ndcg_at_k = np.mean(ndcg_list)\n",
        "    \n",
        "    return recall_at_k, mrr_at_k, ndcg_at_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluando modelo en test set...\n",
            "\n",
            "Resultados en Test Set:\n",
            "Recall@10: 0.6679\n",
            "MRR@10:    0.4930\n",
            "NDCG@10:   0.5344\n"
          ]
        }
      ],
      "source": [
        "# Evaluar en el test set\n",
        "print(\"Evaluando modelo en test set...\")\n",
        "recall_10, mrr_10, ndcg_10 = evaluate_model(model, test_loader, k=10)\n",
        "\n",
        "print(f\"\\nResultados en Test Set:\")\n",
        "print(f\"Recall@10: {recall_10:.4f}\")\n",
        "print(f\"MRR@10:    {mrr_10:.4f}\")\n",
        "print(f\"NDCG@10:   {ndcg_10:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "recsys",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
